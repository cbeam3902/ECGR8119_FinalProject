{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJWyAzVCtmpy"
      },
      "source": [
        "# Final Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGT91obytqPC"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JWlosLklLeo-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-07 15:32:30.656854: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-12-07 15:32:30.680814: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-07 15:32:32.413541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-12-07 15:32:36.275042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:36.305110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:36.305206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu,True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "W9ZY6h9hTlNa"
      },
      "outputs": [],
      "source": [
        "train_GAN = False\n",
        "train_original_classifier = False\n",
        "train_GAN_classifier = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4Fbt8_EL3jC",
        "outputId": "d59af5e6-6ab2-4296-84c7-e36e9b14f1f1"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wwwiBwPSL6p8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-07 15:32:36.985636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:36.985754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:36.985794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:37.030595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:37.030674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:37.030723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-12-07 15:32:37.030766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22157 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 32, 32, 3).astype('float32')\n",
        "train_images = tf.image.resize(train_images, [64, 64])\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "# train_images = (train_images - 0.5) / 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BohYTmJbMDtH"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 128\n",
        "noise_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bv-DzlQOMJVw"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYOVKPSLtczZ"
      },
      "source": [
        "## DCGAN Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dY-1kHM09BAj"
      },
      "outputs": [],
      "source": [
        "def make_generator_model(image_widthheight):\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=0.02)\n",
        "    batch_initializer = tf.keras.initializers.RandomNormal(mean=1, stddev=0.02)\n",
        "    zeror = tf.keras.initializers.Zeros()\n",
        "    model = tf.keras.Sequential()\n",
        "    # model.add(layers.Dense(image_widthheight//8*image_widthheight//8*image_widthheight*4, use_bias=False, input_shape=(100,)))\n",
        "    # model.add(layers.BatchNormalization())\n",
        "    # model.add(layers.ReLU())\n",
        "\n",
        "    # model.add(layers.Reshape((image_widthheight//8, image_widthheight//8, image_widthheight*4)))\n",
        "    # assert model.output_shape == (None, image_widthheight//8, image_widthheight//8, image_widthheight*4)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(image_widthheight*8, (4, 4), strides=(1, 1), use_bias=False, input_shape=(1,1,100), kernel_initializer=initializer))\n",
        "    # assert model.output_shape == (None, image_widthheight//8, image_widthheight//8, image_widthheight*4)\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(image_widthheight*4, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=initializer))\n",
        "    # assert model.output_shape == (None, image_widthheight//4, image_widthheight//4, image_widthheight*2)\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(image_widthheight*2, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=initializer))\n",
        "    # assert model.output_shape == (None, image_widthheight//4, image_widthheight//4, image_widthheight*2)\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(image_widthheight, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=initializer))\n",
        "    # assert model.output_shape == (None, image_widthheight//2, image_widthheight//2, image_widthheight)\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.ReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', use_bias=False, kernel_initializer=initializer, activation='tanh'))\n",
        "    # assert model.output_shape == (None, image_widthheight, image_widthheight, 3)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iDDwMvfP9ssi"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model(image_widthheight):\n",
        "    initializer = tf.keras.initializers.RandomNormal(mean=0, stddev=0.02)\n",
        "    batch_initializer = tf.keras.initializers.RandomNormal(mean=1, stddev=0.02)\n",
        "    zeror = tf.keras.initializers.Zeros()\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(image_widthheight, (4, 4), strides=(2, 2), padding='same', kernel_initializer=initializer,\n",
        "                                     input_shape=[image_widthheight, image_widthheight, 3]))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    model.add(layers.Conv2D(image_widthheight*2, (4, 4), strides=(2, 2), padding='same', kernel_initializer=initializer))\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    model.add(layers.Conv2D(image_widthheight*4, (4, 4), strides=(2, 2), padding='same', kernel_initializer=initializer))\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    model.add(layers.Conv2D(image_widthheight*8, (4, 4), strides=(2, 2), padding='same', kernel_initializer=initializer))\n",
        "    model.add(layers.BatchNormalization(gamma_initializer=batch_initializer, beta_initializer=zeror))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "\n",
        "    model.add(layers.Conv2D(1, (4, 4), strides=(2, 2), kernel_initializer=initializer, activation=\"sigmoid\"))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txS8ZX-W9zNG",
        "outputId": "1b7d7e8e-d342-4525-9fb3-dc28840b22f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_transpose (Conv2DTr  (None, 4, 4, 512)         819200    \n",
            " anspose)                                                        \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 4, 4, 512)         2048      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2D  (None, 8, 8, 256)         2097152   \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 8, 8, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_1 (ReLU)              (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2D  (None, 16, 16, 128)       524288    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_2 (ReLU)              (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2D  (None, 32, 32, 64)        131072    \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 32, 32, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " re_lu_3 (ReLU)              (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_4 (Conv2D  (None, 64, 64, 3)         3072      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3578624 (13.65 MB)\n",
            "Trainable params: 3576704 (13.64 MB)\n",
            "Non-trainable params: 1920 (7.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cbeam18/.local/lib/python3.8/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "generator = make_generator_model(64)\n",
        "\n",
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vx41-19b94ZL",
        "outputId": "9cece62d-1db8-4af1-97c0-06282edfccfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 64)        3136      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 16, 16, 128)       131200    \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 16, 16, 128)       512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 256)         524544    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 512)         2097664   \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 4, 4, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 1, 1)           8193      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2768321 (10.56 MB)\n",
            "Trainable params: 2766529 (10.55 MB)\n",
            "Non-trainable params: 1792 (7.00 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "discriminator = make_discriminator_model(64)\n",
        "\n",
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gLfMncrm99vF"
      },
      "outputs": [],
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5, beta_2=0.999)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZOLEfKww-pWG"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "seed = tf.random.normal([49, 1, 1, noise_dim])\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(7, 7))\n",
        "\n",
        "  for i in range(49):\n",
        "      plt.subplot(7, 7, i+1)\n",
        "      plt.imshow((predictions[i] + 1) / 2)\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('outputs/image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "n0chFpcF-Oc2"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noises = tf.random.normal([BATCH_SIZE, 1, 1, noise_dim])\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noises, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generated_images = generator(noises, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "        g_loss = generator_loss(fake_output)\n",
        "        gen_loss = g_loss\n",
        "\n",
        "    grad_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    grad_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(grad_gen, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(grad_disc, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    resize_layer = tf.keras.layers.Resizing(64, 64)\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        # start = time.time()\n",
        "        for batch in dataset:\n",
        "            gen_loss, disc_loss = train_step(batch)\n",
        "            # train_step(resize_layer(batch))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Generator Loss: {gen_loss}, Discriminator Loss: {disc_loss}\")\n",
        "        generate_and_save_images(generator, epoch, seed)\n",
        "        # print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XAQlBeSZ_NrV",
        "outputId": "dbb3c895-1cb3-47ea-f219-0ed7f7d088a3"
      },
      "outputs": [],
      "source": [
        "if train_GAN:\n",
        "  train(train_dataset, 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OW_A1iZPTHzE"
      },
      "outputs": [],
      "source": [
        "if train_GAN:\n",
        "  generator.save_weights(\"models/generator_64.ckpt\")\n",
        "  discriminator.save_weights(\"models/discriminator_64.ckpt\")\n",
        "else:\n",
        "  generator.load_weights(\"models/generator_64.ckpt\")\n",
        "  discriminator.load_weights(\"models/discriminator_64.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MobileNet V2 - No GAN inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()\n",
        "train_images = train_images / 255\n",
        "train_images = tf.image.resize(train_images, (64, 64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.RandomFlip(\"horizontal\"),\n",
        "        keras.layers.RandomRotation(0.1),\n",
        "        keras.layers.RandomFlip(\"vertical\"),\n",
        "        keras.layers.RandomZoom(height_factor=0.1,width_factor=0.1)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " mobilenetv2_1.00_128 (Func  (None, 4, 4, 1280)        2257984   \n",
            " tional)                                                         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 1280)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                12810     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2270794 (8.66 MB)\n",
            "Trainable params: 12810 (50.04 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "base_mobile_model = keras.applications.MobileNetV2(\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(128,128,3),\n",
        "    include_top=False\n",
        ")\n",
        "\n",
        "base_mobile_model.trainable = False\n",
        "\n",
        "inputs = keras.Input(shape=(64,64,3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "scaled_layer = keras.layers.Rescaling(scale=1/255.0)\n",
        "resize_layer = tf.keras.layers.Resizing(128, 128)\n",
        "x = resize_layer(x)\n",
        "\n",
        "x = base_mobile_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)\n",
        "outputs = keras.layers.Dense(10)(x)\n",
        "mobile_model = keras.Model(inputs, outputs)\n",
        "\n",
        "mobile_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if train_original_classifier:\n",
        "  keras.backend.clear_session()\n",
        "  mobile_model.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['accuracy'],\n",
        "  )\n",
        "\n",
        "  # logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  # tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "  epochs = 20\n",
        "  mobile_model.fit(train_images, train_labels, batch_size=128, epochs=epochs) #, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "if train_original_classifier:\n",
        "  keras.backend.clear_session()\n",
        "  base_mobile_model.trainable = True\n",
        "\n",
        "  mobile_model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-5),\n",
        "      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=['accuracy'],\n",
        "  )\n",
        "\n",
        "  mobile_model.fit(train_images, train_labels, batch_size=128, epochs=10) #, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "if train_original_classifier:\n",
        "  mobile_model.save_weights(\"models/mobile_model_64.ckpt\")\n",
        "else:\n",
        "  mobile_model.load_weights(\"models/mobile_model_64.ckpt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Fake Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "holder_0 = []\n",
        "holder_1 = []\n",
        "holder_2 = []\n",
        "holder_3 = []\n",
        "holder_4 = []\n",
        "holder_5 = []\n",
        "holder_6 = []\n",
        "holder_7 = []\n",
        "holder_8 = []\n",
        "holder_9 = []\n",
        "\n",
        "holder_holder = [holder_0, holder_1, holder_2, holder_3, holder_4, holder_5, holder_6, holder_7, holder_8, holder_9]\n",
        "\n",
        "full_holder = [False, False, False, False, False, False, False, False, False, False]\n",
        "\n",
        "full_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-07 15:32:40.697534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2023-12-07 15:32:40.708338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n"
          ]
        }
      ],
      "source": [
        "gen_number = 100\n",
        "size_per_class = 1000\n",
        "for _ in range(1000):\n",
        "    # Break when done generating images\n",
        "    if all(full_holder):\n",
        "        break\n",
        "    # Generate Images\n",
        "    seed = tf.random.normal([gen_number, 1, 1, noise_dim])\n",
        "    gen_images = generator(seed, training=False)\n",
        "\n",
        "    # Get Labels\n",
        "    preds = mobile_model(gen_images * 0.5 + 0.5, training=False)\n",
        "    labels = tf.reshape(tf.cast(tf.math.argmax(preds, axis=1), dtype=tf.uint8), (gen_number, 1))\n",
        "\n",
        "    # Sort labeled images into respective holder if not full\n",
        "    for idx in range(gen_number):\n",
        "        label = labels[idx][0].numpy()\n",
        "        if not full_holder[label]:\n",
        "            holder_holder[label].append(tf.cast(gen_images[idx] * 127.5 + 127.5, \"uint8\"))\n",
        "            if len(holder_holder[label]) == size_per_class:\n",
        "                full_holder[label] = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "for classIdx in range(10):\n",
        "    for imageIdx in range(size_per_class):\n",
        "        file_path = \"fake_dataset2/%d/%d_%d.png\" % (classIdx, classIdx, imageIdx)\n",
        "        tf.keras.utils.save_img(file_path, holder_holder[classIdx][imageIdx])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
